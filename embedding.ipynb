{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6084335a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x17071e250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Smaller BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3eb4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from CSV...\n",
      "                                            filename  \\\n",
      "0  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "1  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "2  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "3  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "4  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "\n",
      "                                            sentence  \n",
      "0  A novel new 40-million-US dollars health clini...  \n",
      "1  It\\nwill be the world's first detoxification h...  \n",
      "2                                     I kid you not.  \n",
      "3  Dr Somchai Hongnamsakul of Johns Hopkins Unive...  \n",
      "4  Explaining the new therapy, Dr Hongnamsakul sa...  \n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "\n",
    "DF_DIR = Path.cwd() / \"saved_dfs\"\n",
    "DF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_path = DF_DIR / \"dataframe.csv\"\n",
    "\n",
    "# Conditional that checks whether we saved the dfs as csv files in prior run.\n",
    "# If yes, then reinitialise these csvs as dfs.\n",
    "# If not, then create the dfs and save them in csv format for next run.\n",
    "if df_path.exists():\n",
    "    print(\"Loading dataset from CSV...\")\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # Print filename for progress indication\n",
    "            print(filename)\n",
    "\n",
    "            file_path = DATA_DIR / filename\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                # Use spaCy to process the content into sentences\n",
    "                doc = nlp(content)\n",
    "                sentences = [sent.text.strip() for sent in doc.sents]\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    # Append each sentence and filename to a list\n",
    "                    data.append({\"filename\": filename, \"sentence\": sentence})\n",
    "\n",
    "\n",
    "    # Convert the list of sentence/filename dictionaries into a dataframe \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(df_path, index=False)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f929233f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from file...\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "version = 1\n",
    "\n",
    "EMBEDDING_DIR = Path.cwd() / \"embeddings\"\n",
    "EMBEDDING_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "embeddings_path = EMBEDDING_DIR / f'sentence_embeddings_v{version}.joblib'\n",
    "\n",
    "# Conditional to check whether our embeddings joblib already exists from prior runs.\n",
    "if not embeddings_path.exists():\n",
    "    print(\"Generating embeddings for the dataset...\")\n",
    "    embeddings = model.encode(df['sentence'].tolist(), show_progress_bar=True)\n",
    "    joblib.dump(embeddings, embeddings_path)\n",
    "else:\n",
    "    print(\"Loading embeddings from file...\")\n",
    "    embeddings = joblib.load(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd05dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def search(query, embeddings, df):\n",
    "    # Encode the query string using the model to get its embedding.\n",
    "    query_embedding = model.encode([query]) \n",
    "\n",
    "    # Calculate the cosine similarity between the query embedding and all embeddings in the dataset.\n",
    "    # cosine_similarity returns a matrix where each row is the similarity of the query to each document.\n",
    "    # We take the first row [0] because there's only one query, resulting in a one-dimensional array of similarities.\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "\n",
    "    top_indices = np.argsort(similarities)[-20:]\n",
    "\n",
    "    # Use DataFrame.iloc to select the rows at the given indices (top_indices).\n",
    "    # This gives us the rows from the dataframe that correspond to the top 20 similarities.\n",
    "    top_docs = df.iloc[top_indices]\n",
    "\n",
    "    # Select the corresponding top similarity scores using the indices.\n",
    "    # This gives us the actual similarity scores of the top 20 matches.\n",
    "    top_scores = similarities[top_indices]\n",
    "\n",
    "    # Return the top matching documents and their similarity scores.\n",
    "    return top_docs[['sentence', 'filename']], top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a95e0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_columns=[\"Query\", \"Result\", \"Cosine\", \"Filename\"]\n",
    "\n",
    "def create_test_set(query, embeddings, df, test_set):\n",
    "    top_docs, top_scores = search(query, embeddings, df)\n",
    "    \n",
    "    if top_docs.empty:\n",
    "        print(\"No documents found for this query.\")\n",
    "        return test_set\n",
    "\n",
    "    new_rows = []\n",
    "    # Use zip to positionally combine and iterate over the df and scores in parallel. \n",
    "    # iterrows() is used to return a tuple of (index, Series) from the df.\n",
    "    for (index, row), score in zip(top_docs.iterrows(), top_scores):\n",
    "        new_row = {\n",
    "            \"Query\": query,\n",
    "            \"Result\": row['sentence'],\n",
    "            \"Cosine\": score,\n",
    "            \"Filename\": row['filename'],\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "    test_set = pd.concat([test_set, new_rows_df], ignore_index=True)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61276110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_set_from_queries(query_file_path, embeddings, df):\n",
    "    test_set = pd.DataFrame(columns=test_set_columns)\n",
    "    with open(query_file_path, 'r') as file:\n",
    "        queries = file.read().splitlines()\n",
    "    \n",
    "    for query in queries:\n",
    "        test_set = create_test_set(query, embeddings, df, test_set)\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "TEST_SET_DIR = Path.cwd() / \"test_sets\"\n",
    "QUERIES_DIR = Path.cwd() / \"queries\"\n",
    "TEST_SET_DIR.mkdir(exist_ok=True)\n",
    "QUERIES_DIR.mkdir(exist_ok=True)\n",
    "test_set_path = TEST_SET_DIR / f\"test_set_v{version}.csv\"\n",
    "query_file_path = QUERIES_DIR / \"queries.txt\"\n",
    "\n",
    "test_set = generate_test_set_from_queries(query_file_path, embeddings, df)\n",
    "\n",
    "test_set.to_csv(test_set_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae5b34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Query Group  Total Hits  \\\n",
      "0              Is salt unhealthy, Salt damages cells           0   \n",
      "1                     What are signs of intelligence           1   \n",
      "2  What are signs of intelligence, What are signs...           0   \n",
      "3                        How to gain weight quickly            0   \n",
      "4     What is arthritis, What is arthritis caused by           2   \n",
      "\n",
      "                                  Matching Sentences  \n",
      "0                                                     \n",
      "1  All hyperactive children have potential genius...  \n",
      "2                                                     \n",
      "3                                                     \n",
      "4  The cooked saturated fat are a problem, but th...  \n"
     ]
    }
   ],
   "source": [
    "def normalize_sentence(sentence):\n",
    "    # Need to remove any extra spaces/linebreaks\n",
    "    # The original df sentences sometimes keep their raw formatting (weird line breaks)\n",
    "    # This leads to a failure to match the embedding search results to my manually curated results\n",
    "    sentence = sentence.replace(\"\\n\", \" \")\n",
    "    return sentence.strip()\n",
    "\n",
    "def compute_evaluations(test_set, relevant_results):\n",
    "    evaluation_data = []\n",
    "\n",
    "    # My manually selected results (from keyword search)\n",
    "    # are grouped by a set of possible analogous queries.\n",
    "    # This loop iterates over each row that contains the same query group.\n",
    "    # After each row in the matching column has been iterated over, it iterates over the rows for the next query group.\n",
    "    for query_group in relevant_results['Query'].unique():\n",
    "        grouped_queries = query_group.split(',')\n",
    "        # Create a list of the normalised results from the relevant result dataframe (filtered on the query group)\n",
    "        relevant_set = [normalize_sentence(sentence) for sentence in relevant_results[relevant_results['Query'] == query_group]['Result']]\n",
    "        \n",
    "        total_hits = 0\n",
    "        matching_sentences = []\n",
    "\n",
    "        # Iterate through the queries in the query group, e.g. ('What is arthritis', 'What is arthritis caused by')\n",
    "        for query in grouped_queries:\n",
    "            query = query.strip()\n",
    "            # Find all the results from the test_set that pertain to the individual query (results from embedding search)\n",
    "            query_results = test_set[test_set['Query'] == query]['Result'].apply(normalize_sentence)\n",
    "\n",
    "            # Count the sentences that appear in both my manually collated relevant_results and the embedding search results\n",
    "            for sentence in query_results:\n",
    "                if sentence in relevant_set:\n",
    "                    total_hits += 1\n",
    "                    matching_sentences.append(sentence)\n",
    "\n",
    "        evaluation_data.append({\n",
    "            'Query Group': query_group,\n",
    "            'Total Hits': total_hits,\n",
    "            'Matching Sentences': ', '.join(matching_sentences)\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(evaluation_data)\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "EVAL_DIR = Path.cwd() / \"evaluations\"\n",
    "EVAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "relevant_results_path = QUERIES_DIR / \"relevant_query_results.csv\"\n",
    "relevant_results = pd.read_csv(relevant_results_path)\n",
    "\n",
    "test_set_path = TEST_SET_DIR / f\"test_set_v{version}.csv\"\n",
    "test_set = pd.read_csv(test_set_path)\n",
    "\n",
    "eval_df = compute_evaluations(test_set, relevant_results)\n",
    "eval_path = EVAL_DIR / f\"evaluation_v{version}.csv\"\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177e185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
