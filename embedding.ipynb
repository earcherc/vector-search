{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6084335a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1639577d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Smaller BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3eb4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarrhea-based_Detoxification_Hotel_By_Medical_Doctors.txt\n",
      "The_FDA_Approved_5_Viruses_for_Food_Treatment.txt\n",
      "Genius_Children.txt\n",
      "Are_Raw_Miso_And_Shoyu_Healthy_Sauces?.txt\n",
      "Safe_Cutting_Boards.txt\n",
      "Multiple_Lacerations_Healed_Without_Medical_Help.txt\n",
      "Cholesterol,_LDL_and_HDL.txt\n",
      "Can_We_Preserve_Raw_Chicken_In_Vinegar_Or_Lemon_Juice?.txt\n",
      "Abrasions,_Fractures_and_Breaks.txt\n",
      "Is_Raw_Chocolate_Made_From_Whole_Raw_Cocoa_Beans_Addictive_Or_Harmful?.txt\n",
      "What_Is_Constipation_And_How_Do_We_Resolve_It?.txt\n",
      "Our_Ubiquitous_Microbial_Friends.txt\n",
      "Quinton.txt\n",
      "My_Survival_Kit.txt\n",
      "Medical_Propaganda_about_Inflammatory_Breast_Cancer.txt\n",
      "Frozen_Pastured_Meat_VS_Fresh_Supermarket_Meat.txt\n",
      "Dental_Hygiene,_Causes_of_Decay_and_Reversal,_and_Re-enamelization.txt\n",
      "Bad_And_Good_Parasites,_And_Malaria?.txt\n",
      "View_on_Medical_Establishment.txt\n",
      "Formula_for_a_Healthy_Baby.txt\n",
      "Top_Aussie_Doctor_Says_Pick_Your_Nose_And_Eat_it.txt\n",
      "Supplements.txt\n",
      "Is_Raw_Milk_Always_Beneficial_Even_With_Much_Bacteria?.txt\n",
      "How_Long_Does_It_Take_To_Understand_The_Primal_Diet(TM)?.txt\n",
      "Rawesome_Trial_Outcome.txt\n",
      "Whole_Foods_Markets,_Inc._Friend_To_Better_Health_Or_Foe?.txt\n",
      "What_Should_We_Consider_For_Health_When_Buying_A_New_Car?.txt\n",
      "Lobbying_in_Washington,_DC_for_Raw_Milk.txt\n",
      "How_Can_EMFs_Cause_Death_Prematurely?.txt\n",
      "Bruises,_Injuries_and_Pain_-_Do_We_Apply_Ice_Or_Heat?.txt\n",
      "Malaria.txt\n",
      "My_Research_And_Experiments_Questioned.txt\n",
      "Primal_Diet_Is_Not_A_Stagnant_Diet.txt\n",
      "Care_To_Have_A_Piss_Of_A_Drink_With_Me?.txt\n",
      "Natural_Toys,_Oh,_My!.txt\n",
      "At_What_Age_Is_Death_Inevitable?.txt\n",
      "What_Is_Nutrient_Value_Of_Dehydrated_foods?.txt\n",
      "More_Clarity_On_Food-borne_Bacterial_Contamination.txt\n",
      "London_Times_Interview.txt\n",
      "Do_We_Believe_Medical_Studies?.txt\n",
      "Frozen_Fish_VS_Frozen_Land_Animals.txt\n",
      "Paul_Andrews_Interview.txt\n",
      "Benefits_of_Raw_Eggs.txt\n",
      "Study_Of_Thyroid_Cancer.txt\n",
      "What_Are_Drugs_And_Supplements?.txt\n",
      "What_Water_Should_I_Buy?.txt\n",
      "What_Is_Our_Likelihood_Of_Developing_Cancer(s)?.txt\n",
      "Chemicals_Used_to_Protect_Food_From_Bacteria;_Harmful.txt\n",
      "Salt_And_Headaches.txt\n",
      "Toxic_Chemicals_Out-Gassing_Into_Our_Homes.txt\n",
      "Iridology.txt\n",
      "Medications_Are_Toxic_And_Store_In_The_Tissue.txt\n",
      "How_Much_Bacteria_Are_We_Today?.txt\n",
      "LA_Times_Article_About_Raw-Foods.txt\n",
      "Superfoods.txt\n",
      "Does_Raw_Milk_Do_A_Body_Good?.txt\n",
      "Exercise;_The_Good,_Bad_and_Beautiful.txt\n",
      "How_Much_Energy_Should_I_Expect_To_Experience?.txt\n",
      "What_Would_Happen_If_Aajonus_Ate_Some_Cooked_Meat?.txt\n",
      "Eating_Out,_Is_It_Safe?.txt\n",
      "Resolving_Early_Morning_Racing_Mind.txt\n",
      "Medical_Researchers_Proved_90%_Medical_Research_Is_False.txt\n",
      "Fermented_Vegetables;_The_Good,_Bad_and_Stinky.txt\n",
      "Q&A_Of_July_20,_2008.txt\n",
      "Quality_or_Quantity?.txt\n",
      "Grow_In_Height_After_Age_21.txt\n",
      "Pickled_Fish.txt\n",
      "The_Recipe_for_Living_Without_Disease.txt\n",
      "How_To_Use_An_EMF_Meter.txt\n",
      "Interview_on_Talksport.net.txt\n",
      "Depression.txt\n",
      "Ingredients_In_Injections_With_Hair_And_Iridology_Analyses.txt\n",
      "Why_Do_Most_Physicians_Refuse_Chemo-treatments?.txt\n",
      "Child_Cured_by_Mothers_Feces_-_Eat_Shit_and_Live!.txt\n",
      "Oysters_-_Special_Food_In_Our_Toxic_World.txt\n",
      "How_Do_Electromagnetic_Fields_Affect_Us?.txt\n",
      "What_Is_Nutrient_Value_Trace_Minerals?.txt\n",
      "Considering_Chemotherapy_As_An_Option_For_Cancer?.txt\n",
      "Q&A_Of_July_8,_2001.txt\n",
      "Blood_Types_For_Meat.txt\n",
      "Baby_Food_For_Mothers_Who_Can't_Breastfeed.txt\n",
      "Bacteria,_Antibiotics,_Immune_System.txt\n",
      "Ball_and_Kerr_Jar_Lids,_Are_They_Plastic_Coated_and_Toxic_or_Not?.txt\n",
      "Does_Food_Affect_Behavior?.txt\n",
      "Friendly_Bacteria_Protect_Against_Type_1_Diabetes.txt\n",
      "Are_There_Aggressive_Treatments_For_Cancer?.txt\n",
      "Microbe_Food-Poisoning;_Fact_or_Fiction?.txt\n",
      "What_Place_Do_Energy_Therapies_Take_In_Healing?.txt\n",
      "Chemical_Burns_Can_Be_Devastating_But_Managed_And_Healed.txt\n",
      "E.coli.txt\n",
      "Effects_of_Dietary_and_Environmental_Pollution_on_Children's_Sleep.txt\n",
      "Study_Shows_That_Chubby_People_Live_Longest.txt\n",
      "Q&A_Of_March_18,_2012.txt\n",
      "Oxalates.txt\n",
      "Is_It_True_You_Eat_Buckets_Of_Cow_Dung?.txt\n",
      "Can_We_Preserve_Raw_Fish_In_Oil?.txt\n",
      "100%_Of_Fresh-Water_Lakes_And_Streams_Are_Polluted_With_Mercury.txt\n",
      "Non-organic_Meat.txt\n",
      "Man_Eats_Live_Frogs_and_Rats_for_Health.txt\n",
      "Dangers_Of_Salt.txt\n",
      "Iron_On_The_Primal_Diet,_Is_It_A_Problem?.txt\n",
      "Beneficial_Home_Baths.txt\n",
      "New_Source_Of_Stem_Cells_-_Mouse_Sperm.txt\n",
      "Athletes_And_Longevity_On_Primal_Diet.txt\n",
      "What_Role_Do_Genetics_and_Microbes_Play_In_Disease?.txt\n",
      "Benzene,_Cancer_and_Soft_Drinks_Connection.txt\n",
      "Primal_Diet_Workshop_(Part_3).txt\n",
      "High_Blood-Pressure.txt\n",
      "Rae_Bradbury_Interview_1.txt\n",
      "Is_It_Good_To_Donate_To_Charities_That_Feed_The_Poor_and_Starving?.txt\n",
      "Do_You_Buy_Chicken_While_Traveling?.txt\n",
      "How_Bad_Are_MRIs?.txt\n",
      "Arsenic_In_Poultry_Meat_And_Eggs.txt\n",
      "We_Want_To_Live.txt\n",
      "Soy_Toxicity_In_Poultry_Meat_And_Eggs.txt\n",
      "Gum_And_Tooth_Disease.txt\n",
      "Rae_Bradbury_Interview_2.txt\n",
      "                                            filename  \\\n",
      "0  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "1  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "2  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "3  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "4  Diarrhea-based_Detoxification_Hotel_By_Medical...   \n",
      "\n",
      "                                            sentence  line_number  \n",
      "0  A novel new 40-million-US dollars health clini...            1  \n",
      "1  It\\nwill be the world's first detoxification h...            2  \n",
      "2                                     I kid you not.            4  \n",
      "3  Dr Somchai Hongnamsakul of Johns Hopkins Unive...            4  \n",
      "4  Explaining the new therapy, Dr Hongnamsakul sa...            8  \n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "\n",
    "DF_DIR = Path.cwd() / \"saved_dfs\"\n",
    "DF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_path = DF_DIR / \"dataframe.csv\"\n",
    "\n",
    "# Conditional that checks whether we saved the dfs as csv files in prior run.\n",
    "# If yes, then reinitialise these csvs as dfs.\n",
    "# If not, then create the dfs and save them in csv format for next run.\n",
    "if df_path.exists():\n",
    "    print(\"Loading dataset from CSV...\")\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "\n",
    "            file_path = DATA_DIR / filename\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Create a mapping of character positions to line numbers\n",
    "            line_starts = {0: 1}\n",
    "            for i, char in enumerate(content):\n",
    "                if char == '\\n':\n",
    "                    line_starts[i + 1] = line_starts[i] + 1\n",
    "                else:\n",
    "                    line_starts[i + 1] = line_starts[i]\n",
    "\n",
    "            # Process the entire content with spaCy\n",
    "            doc = nlp(content)\n",
    "            for sent in doc.sents:\n",
    "                start_char = sent.start_char\n",
    "                line_number = line_starts[start_char]\n",
    "                sentence = sent.text.strip()\n",
    "                data.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"sentence\": sentence,\n",
    "                    \"line_number\": line_number\n",
    "                })\n",
    "\n",
    "\n",
    "    # Convert the list of sentence/filename dictionaries into a dataframe \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(df_path, index=False)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f929233f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5043b0798f534fba96659abf2989a86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "EMBEDDING_DIR = Path.cwd() / \"embeddings\"\n",
    "EMBEDDING_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "embeddings_path = EMBEDDING_DIR / f'sentence_embeddings.joblib'\n",
    "\n",
    "# Conditional to check whether our embeddings joblib already exists from prior runs.\n",
    "if not embeddings_path.exists():\n",
    "    print(\"Generating embeddings for the dataset...\")\n",
    "    embeddings = model.encode(df['sentence'].tolist(), show_progress_bar=True)\n",
    "    joblib.dump(embeddings, embeddings_path)\n",
    "else:\n",
    "    print(\"Loading embeddings from file...\")\n",
    "    embeddings = joblib.load(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd05dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def search(query, embeddings, df):\n",
    "    # Encode the query string using the model to get its embedding.\n",
    "    query_embedding = model.encode([query]) \n",
    "\n",
    "    # Calculate the cosine similarity between the query embedding and all embeddings in the dataset.\n",
    "    # cosine_similarity returns a matrix where each row is the similarity of the query to each document.\n",
    "    # We take the first row [0] because there's only one query, resulting in a one-dimensional array of similarities.\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "\n",
    "    top_indices = np.argsort(similarities)[-20:]\n",
    "\n",
    "    # Use DataFrame.iloc to select the rows at the given indices (top_indices).\n",
    "    # This gives us the rows from the dataframe that correspond to the top 20 similarities.\n",
    "    top_docs = df.iloc[top_indices]\n",
    "\n",
    "    # Select the corresponding top similarity scores using the indices.\n",
    "    # This gives us the actual similarity scores of the top 20 matches.\n",
    "    top_scores = similarities[top_indices]\n",
    "\n",
    "    # Return the top matching documents and their similarity scores.\n",
    "    return top_docs[['sentence', 'filename']], top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95e0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_columns=[\"Query\", \"Result\", \"Cosine\", \"Filename\"]\n",
    "\n",
    "def create_test_set(query, embeddings, df, test_set):\n",
    "    top_docs, top_scores = search(query, embeddings, df)\n",
    "    \n",
    "    if top_docs.empty:\n",
    "        print(\"No documents found for this query.\")\n",
    "        return test_set\n",
    "\n",
    "    new_rows = []\n",
    "    # Use zip to positionally combine and iterate over the df and scores in parallel. \n",
    "    # iterrows() is used to return a tuple of (index, Series) from the df.\n",
    "    for (index, row), score in zip(top_docs.iterrows(), top_scores):\n",
    "        new_row = {\n",
    "            \"Query\": query,\n",
    "            \"Result\": row['sentence'],\n",
    "            \"Cosine\": score,\n",
    "            \"Filename\": row['filename'],\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "    test_set = pd.concat([test_set, new_rows_df], ignore_index=True)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61276110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_set_from_queries(query_file_path, embeddings, df):\n",
    "    test_set = pd.DataFrame(columns=test_set_columns)\n",
    "    with open(query_file_path, 'r') as file:\n",
    "        queries = file.read().splitlines()\n",
    "    \n",
    "    for query in queries:\n",
    "        test_set = create_test_set(query, embeddings, df, test_set)\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "TEST_SET_DIR = Path.cwd() / \"test_sets\"\n",
    "QUERIES_DIR = Path.cwd() / \"queries\"\n",
    "TEST_SET_DIR.mkdir(exist_ok=True)\n",
    "QUERIES_DIR.mkdir(exist_ok=True)\n",
    "test_set_path = TEST_SET_DIR / f\"test_set.csv\"\n",
    "query_file_path = QUERIES_DIR / \"queries.txt\"\n",
    "\n",
    "test_set = generate_test_set_from_queries(query_file_path, embeddings, df)\n",
    "\n",
    "test_set.to_csv(test_set_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae5b34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Query Group  Total Hits  \\\n",
      "0              Is salt unhealthy, Salt damages cells           0   \n",
      "1                     What are signs of intelligence           1   \n",
      "2  What are signs of intelligence, What are signs...           0   \n",
      "3                        How to gain weight quickly            0   \n",
      "4     What is arthritis, What is arthritis caused by           2   \n",
      "\n",
      "                                  Matching Sentences  \n",
      "0                                                     \n",
      "1  All hyperactive children have potential genius...  \n",
      "2                                                     \n",
      "3                                                     \n",
      "4  The cooked saturated fat are a problem, but th...  \n"
     ]
    }
   ],
   "source": [
    "def normalize_sentence(sentence):\n",
    "    # Need to remove any extra spaces/linebreaks\n",
    "    # The original df sentences sometimes keep their raw formatting (weird line breaks)\n",
    "    # This leads to a failure to match the embedding search results to my manually curated results\n",
    "    sentence = sentence.replace(\"\\n\", \" \")\n",
    "    return sentence.strip()\n",
    "\n",
    "def compute_evaluations(test_set, relevant_results):\n",
    "    evaluation_data = []\n",
    "\n",
    "    # My manually selected results (from keyword search)\n",
    "    # are grouped by a set of possible analogous queries.\n",
    "    # This loop iterates over each row that contains the same query group.\n",
    "    # After each row in the matching column has been iterated over, it iterates over the rows for the next query group.\n",
    "    for query_group in relevant_results['Query'].unique():\n",
    "        grouped_queries = query_group.split(',')\n",
    "        # Create a list of the normalised results from the relevant result dataframe (filtered on the query group)\n",
    "        relevant_set = [normalize_sentence(sentence) for sentence in relevant_results[relevant_results['Query'] == query_group]['Result']]\n",
    "        \n",
    "        total_hits = 0\n",
    "        matching_sentences = []\n",
    "\n",
    "        # Iterate through the queries in the query group, e.g. ('What is arthritis', 'What is arthritis caused by')\n",
    "        for query in grouped_queries:\n",
    "            query = query.strip()\n",
    "            # Find all the results from the test_set that pertain to the individual query (results from embedding search)\n",
    "            query_results = test_set[test_set['Query'] == query]['Result'].apply(normalize_sentence)\n",
    "\n",
    "            # Count the sentences that appear in both my manually collated relevant_results and the embedding search results\n",
    "            for sentence in query_results:\n",
    "                if sentence in relevant_set:\n",
    "                    total_hits += 1\n",
    "                    matching_sentences.append(sentence)\n",
    "\n",
    "        evaluation_data.append({\n",
    "            'Query Group': query_group,\n",
    "            'Total Hits': total_hits,\n",
    "            'Matching Sentences': ', '.join(matching_sentences)\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(evaluation_data)\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "EVAL_DIR = Path.cwd() / \"evaluations\"\n",
    "EVAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "relevant_results_path = QUERIES_DIR / \"relevant_query_results.csv\"\n",
    "relevant_results = pd.read_csv(relevant_results_path)\n",
    "\n",
    "test_set_path = TEST_SET_DIR / f\"test_set.csv\"\n",
    "test_set = pd.read_csv(test_set_path)\n",
    "\n",
    "eval_df = compute_evaluations(test_set, relevant_results)\n",
    "eval_path = EVAL_DIR / f\"evaluation.csv\"\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177e185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
